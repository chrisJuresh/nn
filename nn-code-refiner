import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import time

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, num_convs):
        super(Block, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.mlp = nn.Linear(in_channels, num_convs)

        self.convs = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) for _ in range(num_convs)])

    def forward(self, x):
        x_avg = self.avgpool(x).view(x.size(0), -1)
        a = torch.sigmoid(self.mlp(x_avg))
        conv_outputs = [conv(x) for conv in self.convs]
        x = sum(a[:, i].view(-1, 1, 1, 1) * conv_outputs[i] for i in range(len(conv_outputs)))
        return x

class Net(nn.Module):
    def __init__(self, num_blocks, num_convs, num_classes):
        super(Net, self).__init__()
        block_in_channels = 3
        block_out_channels = 16

        self.blocks = nn.ModuleList([Block(block_in_channels if i == 0 else block_out_channels, block_out_channels, num_convs) for i in range(num_blocks)])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Sequential(
            nn.Linear(block_out_channels, 120),
            nn.ReLU(),
            nn.Linear(120, num_classes)
        )

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        x = self.avgpool(x).view(x.size(0), -1)
        x = self.classifier(x)
        return x

# Function to get data loaders
def get_dataloaders(batch_size):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    return trainloader, testloader

import math

# Training function
def train_model(model, trainloader, criterion, optimizer, num_epochs, device, scheduler=None, early_stopping_patience=None):
    # For early stopping
    best_val_loss = float('inf')
    epochs_without_improvement = 0
    
    for epoch in range(num_epochs): 
        model.train()  # Set model to training mode
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        
        # Scheduler step
        if scheduler:
            scheduler.step()
        
        # Print epoch loss
        epoch_loss = running_loss / len(trainloader)
        if math.isnan(epoch_loss):
            raise ValueError("Model diverged.")
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')

def evaluate_model(model, testloader, device):
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f'Accuracy: {accuracy:.2f}%')
    return accuracy

import os
import json
import traceback  # For detailed error logging

def save_state(state, filepath):
    with open(filepath, 'w') as f:
        json.dump(state, f)

def load_state(filepath):
    if os.path.exists(filepath):
        with open(filepath, 'r') as f:
            return json.load(f)
    return None

def run_experiments():
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    log_file_path = "./experiment_logs.txt"
    
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

    state_filepath = "./experiment_state.json"
    os.makedirs(os.path.dirname(state_filepath), exist_ok=True)

    # Load saved state if exists
    saved_state = load_state(state_filepath)
    if saved_state:
        hp = saved_state['best_hp']
        adjustments = saved_state['adjustments']
        best_accuracy = saved_state['best_accuracy']
        best_hp = saved_state['best_hp']
        start_cycle = saved_state['cycle'] + 1  # Start from the next cycle
        best_time = saved_state.get('best_time', float('inf'))  # Initialize if not present
    else:
        # Initial setup if no saved state
        hp = {'lr': 0.001, 'batch_size': 32, 'momentum': 0.95, 'epochs': 2, 'num_blocks': 6, 'num_convs': 24}
        adjustments = {'lr': 0.00129, 'batch_size': 4, 'momentum': 0.05, 'epochs': 1, 'num_blocks': 1, 'num_convs': 2}
        best_accuracy = 0
        best_hp = hp.copy()
        start_cycle = 0
        best_time = float('inf')

    feedback_multiplier = 1.2

    for cycle in range(start_cycle, 100000):
        for param in hp.keys():
            current_adjustment = adjustments[param]
            for direction in [1, -1]:
                adjustment_message = f"Cycle {cycle+1}, adjusting {param}: {direction * current_adjustment}"
                print(adjustment_message)
            
                # Apply and validate adjustment
                new_value = hp[param] + direction * current_adjustment
                
                if param == 'batch_size' or param == 'num_blocks' or param == 'num_convs':
                    # First, round the new value to ensure it's an integer
                    new_value = round(new_value)
                    # Then, apply minimum and maximum constraints where applicable
                    if param == 'batch_size':
                        # For batch_size, ensure it's within [4, 512]
                        new_value = max(4, min(new_value, 512))
                    else:
                        # For num_blocks and num_convs, ensure at least 1
                        new_value = max(1, new_value)
                elif param == 'lr':
                    # Ensure lr is positive and not too close to zero
                    new_value = max(1e-6, new_value)
                elif param == 'momentum':
                    # Ensure momentum is within [0.1, 0.99]
                    new_value = max(0.1, min(new_value, 0.99))
                
                
                hp[param] = new_value  # Update the hyperparameter with the validated new value

                accuracy = -1  # Or another sensible default value indicating it hasn't been set yet
                training_time = float('inf')  #
                start_time = time.time()
                try:
                    # Perform training and evaluation
                    trainloader, testloader = get_dataloaders(round(hp['batch_size']))
                    model = Net((round(hp['num_blocks'])), (round(hp['num_convs'])), 10).to(device)
                    criterion = nn.CrossEntropyLoss()
                    optimizer = optim.SGD(model.parameters(), lr=hp['lr'], momentum=hp['momentum'])
                    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

                    train_model(model, trainloader, criterion, optimizer, (round(hp['epochs'])), device, scheduler)
                    accuracy = evaluate_model(model, testloader, device)

                except Exception as e:
                    with open("error_logs.txt", "a") as error_log:
                        error_log.write(f"Error on cycle {cycle+1}, param {param}: {str(e)}\n")
                        traceback.print_exc(file=error_log)  # Log stack trace for debugging
                        
                    print(f"Encountered an error during cycle {cycle+1} with {param}. See error_logs.txt for details.")

                adjustment_decay = 0.01  # Apply a decay to the adjustment when no improvement is found
                no_improvement_limit = 72  # Limit for consecutive non-improvements
                no_improvement_counts = {param: 0 for param in adjustments} 
                
                training_time = time.time() - start_time
                print(f"Training time: {training_time:.2f} seconds")
                # Evaluate and adjust
                if accuracy > best_accuracy or (accuracy == best_accuracy and training_time < best_time):
                    improvement_message = f"Improvement with {param} = {hp[param]}: {accuracy}% accuracy\n"
                    print(improvement_message)
                    best_accuracy = accuracy
                    best_hp = hp.copy()
                    best_time = training_time
                    adjustments[param] *= (feedback_multiplier + 0.3)
                else:
                    if training_time > best_time+30:
                        adjustments[param] *= 0.1
                        print(f"Training time is too long. Adjusting strategy for {param}.")
                    hp[param] -= direction * current_adjustment 
                    no_improvement_counts[param] += 1  
                    adjustments[param] *= feedback_multiplier
                    if no_improvement_counts[param] >= no_improvement_limit:
                        print(f"No improvement for {param} after {no_improvement_limit} attempts. Adjusting strategy.")
                        adjustments[param] *= adjustment_decay
                        no_improvement_counts[param] = 0  

                state = {
                    'hp': hp,
                    'adjustments': adjustments,
                    'best_accuracy': best_accuracy,
                    'best_hp': best_hp,
                    'cycle': cycle,
                    'best_time': best_time
                }
                save_state(state, state_filepath)

                # Write results to file
                with open(log_file_path, "a") as log_file:
                    log_file.write(adjustment_message)
                    log_file.write(improvement_message if accuracy > best_accuracy else "")
                    log_file.write(f"Current best: {best_hp}, accuracy: {best_accuracy}%, best time: {best_time}\n")
                    log_file.write("-" * 50 + "\n")

           # adjustments[param] *= 0.9

    final_message = f"Final best configuration: {best_hp} with accuracy: {best_accuracy}%"
    print(final_message)
    save_state(state, state_filepath) 
    with open(log_file_path, "a") as log_file:
        log_file.write(final_message)

if __name__ == "__main__":
    run_experiments()

